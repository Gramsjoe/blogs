{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be947436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be16333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::940119374655:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker bucket: sagemaker-us-east-1-940119374655\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701414c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "teacher_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "dataset_id = \"imdb\"\n",
    "s3_prefix_dataset = \"knowledge_distill_mistral_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43811c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f's3://{sess.default_bucket()}/{s3_prefix_dataset}'\n",
    "train_dataset_path = dataset_path + '/dataset/train'\n",
    "test_dataset_path = dataset_path + '/dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = '18.py'\n",
    "train_name = script.replace('_', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada920de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-2024-03-31-17-51-56\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{train_name[:-3]}-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58cddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "        'teacher_model_id': teacher_id,\n",
    "        'student_model_id': student_id,\n",
    "        'num_epochs': 1,\n",
    "        'lr': 6e-5,\n",
    "        'fp16': True,\n",
    "        'temperature': 4.0,\n",
    "        'alpha': 0.5,\n",
    "        'save_strategy': 'epoch',\n",
    "        'evaluation_strategy': 'epoch',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3bd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {\n",
    "    \"torch_distributed\":{\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"smdistributed\": {\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\": True,\n",
    "            \"parameters\": {\n",
    "                \"hybrid_shard_degree\": 0,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5153ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    entry_point          = script,\n",
    "    source_dir           = 'scripts',\n",
    "    instance_type        = 'ml.p4d.24xlarge',\n",
    "    instance_count       = 2,\n",
    "    base_job_name        = job_name,\n",
    "    role                 = role,\n",
    "    framework_version    = '2.2.0',\n",
    "    py_version           = 'py310',\n",
    "    hyperparameters      = hyperparameters,\n",
    "    distribution         = distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712fd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_options = {\n",
    "#     \"enabled\" : True,\n",
    "#     \"processes_per_host\" : 8,\n",
    "# }\n",
    "# smp_options = {\n",
    "#     \"enabled\":True,\n",
    "#     \"parameters\": {\n",
    "#         \"microbatches\": 4,\n",
    "#         \"placement_strategy\": \"spread\",\n",
    "#         \"pipeline\": \"interleaved\",\n",
    "#         \"optimize\": \"speed\",\n",
    "#         \"partitions\": 4,\n",
    "#         \"ddp\": True,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# distribution={\n",
    "#     \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "#     \"mpi\": mpi_options\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25efe2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_estimator = HuggingFace(\n",
    "#     entry_point          = script,\n",
    "#     source_dir           = 'scripts',\n",
    "#     instance_type        = 'ml.p4d.24xlarge',\n",
    "#     instance_count       = 2,\n",
    "#     base_job_name        = job_name,\n",
    "#     role                 = role,\n",
    "#     transformers_version = '4.36',\n",
    "#     pytorch_version      = '2.1',\n",
    "#     py_version           = 'py310',\n",
    "#     hyperparameters      = hyperparameters,\n",
    "#     distribution         = distribution\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf68dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: 18-2024-03-31-17-51-56-2024-03-31-17-51-56-423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-31 17:51:57 Starting - Starting the training job\n",
      "2024-03-31 17:51:57 Pending - Training job waiting for capacity......\n",
      "2024-03-31 17:52:43 Pending - Preparing the instances for training...........................\n",
      "2024-03-31 17:57:05 Downloading - Downloading input data...\n",
      "2024-03-31 17:57:45 Downloading - Downloading the training image............\n",
      "2024-03-31 17:59:50 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:40,098 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:40,200 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:40,209 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:40,211 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:40,211 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:40,109 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:40,204 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:40,214 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:40,215 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:40,215 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:41,462 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:41,508 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting typing==3.7.4.3 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading typing-3.7.4.3.tar.gz (78 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing==3.7.4.3 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading typing-3.7.4.3.tar.gz (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 4.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets==2.16.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.16.1)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==1.4.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses==0.6 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.37.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.37.1)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (0.4.2)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 7.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets==2.16.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.16.1)\u001b[0m\n",
      "\u001b[35mCollecting scikit-learn==1.4.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mCollecting dataclasses==0.6 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers==4.37.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.37.1)\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (0.21.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (1.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (1.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements.txt (line 3)) (3.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (2023.12.25)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (0.15.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.1->-r requirements.txt (line 5)) (0.4.2)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1->-r requirements.txt (line 2)) (4.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 80.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: typing\u001b[0m\n",
      "\u001b[34mBuilding wheel for typing (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for typing (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26307 sha256=29f7c1fb694d034677d2214ddf9f8df75400ffb7c4da967e14602f2fa088b5b1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\u001b[0m\n",
      "\u001b[34mSuccessfully built typing\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1->-r requirements.txt (line 2)) (4.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[35mDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 105.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 15.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: typing\u001b[0m\n",
      "\u001b[35mBuilding wheel for typing (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for typing (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26307 sha256=29f7c1fb694d034677d2214ddf9f8df75400ffb7c4da967e14602f2fa088b5b1\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\u001b[0m\n",
      "\u001b[35mSuccessfully built typing\u001b[0m\n",
      "\u001b[34mInstalling collected packages: dataclasses, typing, scikit-learn, responses, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.4.1.post1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.4.1.post1:\u001b[0m\n",
      "\u001b[35mInstalling collected packages: dataclasses, typing, scikit-learn, responses, evaluate\u001b[0m\n",
      "\u001b[35mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[35mFound existing installation: scikit-learn 1.4.1.post1\u001b[0m\n",
      "\u001b[35mUninstalling scikit-learn-1.4.1.post1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.4.1.post1\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled scikit-learn-1.4.1.post1\u001b[0m\n",
      "\u001b[34mSuccessfully installed dataclasses-0.6 evaluate-0.4.1 responses-0.18.0 scikit-learn-1.4.0 typing-3.7.4.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:46,786 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:46,786 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35mSuccessfully installed dataclasses-0.6 evaluate-0.4.1 responses-0.18.0 scikit-learn-1.4.0 typing-3.7.4.3\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:46,802 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:46,802 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:46,938 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:46,903 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:47,019 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:47,028 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:47,132 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-31 18:00:47,142 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"alpha\": 0.5,\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"fp16\": true,\n",
      "        \"lr\": 6e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"hybrid_shard_degree\": 0,\n",
      "            \"tensor_parallel_degree\": 8\n",
      "        },\n",
      "        \"num_epochs\": 1,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"student_model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
      "        \"teacher_model_id\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
      "        \"temperature\": 4.0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"18-2024-03-31-17-51-56-2024-03-31-17-51-56-423\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"18\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"18.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"alpha\":0.5,\"evaluation_strategy\":\"epoch\",\"fp16\":true,\"lr\":6e-05,\"mp_parameters\":{\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8},\"num_epochs\":1,\"save_strategy\":\"epoch\",\"student_model_id\":\"mistralai/Mistral-7B-v0.1\",\"teacher_model_id\":\"mistralai/Mixtral-8x7B-v0.1\",\"temperature\":4.0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=18.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=18\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"alpha\":0.5,\"evaluation_strategy\":\"epoch\",\"fp16\":true,\"lr\":6e-05,\"mp_parameters\":{\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8},\"num_epochs\":1,\"save_strategy\":\"epoch\",\"student_model_id\":\"mistralai/Mistral-7B-v0.1\",\"teacher_model_id\":\"mistralai/Mixtral-8x7B-v0.1\",\"temperature\":4.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"18-2024-03-31-17-51-56-2024-03-31-17-51-56-423\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\",\"module_name\":\"18\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"18.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--alpha\",\"0.5\",\"--evaluation_strategy\",\"epoch\",\"--fp16\",\"True\",\"--lr\",\"6e-05\",\"--mp_parameters\",\"hybrid_shard_degree=0,tensor_parallel_degree=8\",\"--num_epochs\",\"1\",\"--save_strategy\",\"epoch\",\"--student_model_id\",\"mistralai/Mistral-7B-v0.1\",\"--teacher_model_id\",\"mistralai/Mixtral-8x7B-v0.1\",\"--temperature\",\"4.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ALPHA=0.5\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_LR=6e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_STUDENT_MODEL_ID=mistralai/Mistral-7B-v0.1\u001b[0m\n",
      "\u001b[34mSM_HP_TEACHER_MODEL_ID=mistralai/Mixtral-8x7B-v0.1\u001b[0m\n",
      "\u001b[34mSM_HP_TEMPERATURE=4.0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 18.py --alpha 0.5 --evaluation_strategy epoch --fp16 True --lr 6e-05 --mp_parameters hybrid_shard_degree=0,tensor_parallel_degree=8 --num_epochs 1 --save_strategy epoch --student_model_id mistralai/Mistral-7B-v0.1 --teacher_model_id mistralai/Mixtral-8x7B-v0.1 --temperature 4.0\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:47,050 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:47,060 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:47,155 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-31 18:00:47,165 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"alpha\": 0.5,\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"fp16\": true,\n",
      "        \"lr\": 6e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"hybrid_shard_degree\": 0,\n",
      "            \"tensor_parallel_degree\": 8\n",
      "        },\n",
      "        \"num_epochs\": 1,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"student_model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
      "        \"teacher_model_id\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
      "        \"temperature\": 4.0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"18-2024-03-31-17-51-56-2024-03-31-17-51-56-423\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"18\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"18.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"alpha\":0.5,\"evaluation_strategy\":\"epoch\",\"fp16\":true,\"lr\":6e-05,\"mp_parameters\":{\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8},\"num_epochs\":1,\"save_strategy\":\"epoch\",\"student_model_id\":\"mistralai/Mistral-7B-v0.1\",\"teacher_model_id\":\"mistralai/Mixtral-8x7B-v0.1\",\"temperature\":4.0}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=18.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=18\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"alpha\":0.5,\"evaluation_strategy\":\"epoch\",\"fp16\":true,\"lr\":6e-05,\"mp_parameters\":{\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8},\"num_epochs\":1,\"save_strategy\":\"epoch\",\"student_model_id\":\"mistralai/Mistral-7B-v0.1\",\"teacher_model_id\":\"mistralai/Mixtral-8x7B-v0.1\",\"temperature\":4.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"18-2024-03-31-17-51-56-2024-03-31-17-51-56-423\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-940119374655/18-2024-03-31-17-51-56-2024-03-31-17-51-56-423/source/sourcedir.tar.gz\",\"module_name\":\"18\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"18.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--alpha\",\"0.5\",\"--evaluation_strategy\",\"epoch\",\"--fp16\",\"True\",\"--lr\",\"6e-05\",\"--mp_parameters\",\"hybrid_shard_degree=0,tensor_parallel_degree=8\",\"--num_epochs\",\"1\",\"--save_strategy\",\"epoch\",\"--student_model_id\",\"mistralai/Mistral-7B-v0.1\",\"--teacher_model_id\",\"mistralai/Mixtral-8x7B-v0.1\",\"--temperature\",\"4.0\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_ALPHA=0.5\u001b[0m\n",
      "\u001b[35mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[35mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[35mSM_HP_LR=6e-05\u001b[0m\n",
      "\u001b[35mSM_HP_MP_PARAMETERS={\"hybrid_shard_degree\":0,\"tensor_parallel_degree\":8}\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[35mSM_HP_STUDENT_MODEL_ID=mistralai/Mistral-7B-v0.1\u001b[0m\n",
      "\u001b[35mSM_HP_TEACHER_MODEL_ID=mistralai/Mixtral-8x7B-v0.1\u001b[0m\n",
      "\u001b[35mSM_HP_TEMPERATURE=4.0\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 18.py --alpha 0.5 --evaluation_strategy epoch --fp16 True --lr 6e-05 --mp_parameters hybrid_shard_degree=0,tensor_parallel_degree=8 --num_epochs 1 --save_strategy epoch --student_model_id mistralai/Mistral-7B-v0.1 --teacher_model_id mistralai/Mixtral-8x7B-v0.1 --temperature 4.0\u001b[0m\n",
      "\u001b[34m[2024-03-31 18:00:48,197] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-03-31 18:00:48,197] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-03-31 18:00:48,197] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-03-31 18:00:48,197] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:00:48,229] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[35m[2024-03-31 18:00:48,229] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:00:48,229] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m[2024-03-31 18:00:48,229] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank12]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank12]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank12]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank5]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[2024-03-31 18:01:00.078: I torch/sagemaker/state_handler.py:148] Sizes (pp, rep, sdp, tp, world) = (1, None, 0, 8, 16).\u001b[0m\n",
      "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank0]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[2024-03-31 18:01:00.081: I torch/sagemaker/state_handler.py:177] Rewrite sizes (pp, rep, sdp, tp, world) = (1, None, 2, 8, 16).\u001b[0m\n",
      "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank6]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank1]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank4]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank3]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank2]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[34m[rank7]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank10]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank10]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank10]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank8]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank8]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank8]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank9]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank9]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank9]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank13]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank13]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank13]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank14]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank14]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank14]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank11]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank11]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank11]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank15]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank15]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35m[rank15]:[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:54, 19.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:47, 19.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:45, 19.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:47, 19.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:44, 19.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   5%|▌         | 1/19 [00:19<05:55, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.38s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:46, 19.28s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:49, 19.41s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:   5%|▌         | 1/19 [00:19<05:48, 19.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:35, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:38<05:32, 19.55s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.62s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:36, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.60s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.64s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:32, 19.59s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:32, 19.58s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:32, 19.55s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.61s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.60s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.60s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.60s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  11%|█         | 2/19 [00:39<05:33, 19.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [01:00<05:21, 20.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:19, 19.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:19, 19.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.04s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.04s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  16%|█▌        | 3/19 [01:00<05:21, 20.12s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.01s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.02s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:19, 20.00s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.02s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.02s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.03s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.03s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  16%|█▌        | 3/19 [00:59<05:20, 20.04s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.67s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.68s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.69s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.68s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.69s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.69s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.69s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  21%|██        | 4/19 [01:18<04:55, 19.70s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:56, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:56, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:56, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:57, 19.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:56, 19.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:57, 19.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:18<04:56, 19.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  21%|██        | 4/19 [01:19<04:57, 19.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.94s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:38, 19.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:38, 19.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.94s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.95s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:39, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  26%|██▋       | 5/19 [01:39<04:40, 20.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:21, 20.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:21, 20.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:21, 20.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:21, 20.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:21, 20.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [02:00<04:22, 20.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [02:00<04:22, 20.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  32%|███▏      | 6/19 [01:59<04:22, 20.16s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.24s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.26s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.25s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  32%|███▏      | 6/19 [02:00<04:23, 20.26s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:20<04:01, 20.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:20<04:01, 20.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.11s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.98s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 19.99s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  37%|███▋      | 7/19 [02:19<03:59, 20.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 7/19 [02:19<04:01, 20.12s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.74s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.74s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  42%|████▏     | 8/19 [02:38<03:37, 19.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.37s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.37s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:41<03:44, 20.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:41<03:44, 20.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  42%|████▏     | 8/19 [02:40<03:44, 20.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:18, 19.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:18, 19.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:18, 19.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [03:00<03:18, 19.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:18, 19.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:18, 19.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:19, 19.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 9/19 [02:59<03:19, 19.90s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.05s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.05s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.05s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.07s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  47%|████▋     | 9/19 [02:59<03:20, 20.07s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.79s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:58, 19.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:18<02:57, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:57, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 10/19 [03:19<02:58, 19.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.66s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.66s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:37, 19.69s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  58%|█████▊    | 11/19 [03:38<02:38, 19.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 12/19 [03:52<02:05, 17.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  63%|██████▎   | 12/19 [03:55<02:12, 18.94s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.89s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.90s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.90s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  68%|██████▊   | 13/19 [04:14<01:53, 18.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  68%|██████▊   | 13/19 [04:15<01:56, 19.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.76s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.71s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.71s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.71s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  74%|███████▎  | 14/19 [04:32<01:33, 18.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.82s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.79s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.78s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.78s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.79s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.79s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  79%|███████▉  | 15/19 [04:51<01:15, 18.80s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.07s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.06s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.07s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.07s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  84%|████████▍ | 16/19 [05:11<00:57, 19.07s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:14<01:00, 20.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:14<01:00, 20.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:14<01:00, 20.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:15<01:00, 20.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:15<01:00, 20.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:14<01:00, 20.24s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:15<01:00, 20.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  84%|████████▍ | 16/19 [05:15<01:00, 20.23s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.38s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.38s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.38s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.15s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.39s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:27<00:36, 18.39s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.39s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  89%|████████▉ | 17/19 [05:28<00:36, 18.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:41<00:16, 16.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:41<00:16, 16.84s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  95%|█████████▍| 18/19 [05:42<00:16, 16.84s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.63s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  95%|█████████▍| 18/19 [05:43<00:17, 17.64s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.19s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.21s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.21s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.21s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.15s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.23s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.22s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 18.24s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 19/19 [06:03<00:00, 19.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<00:54,  3.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<00:54,  3.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:07,  3.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:06,  3.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:06,  3.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:06,  3.67s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:06,  3.68s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:11,  3.95s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:10,  3.93s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:10,  3.92s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:09,  3.87s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:11,  3.97s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:10,  3.92s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   5%|▌         | 1/19 [00:03<01:10,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:06<00:53,  3.15s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:06<00:54,  3.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:06,  3.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:06,  3.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:06,  3.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:06,  3.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:06,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:05,  3.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:05,  3.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:09<00:51,  3.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:03,  4.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  16%|█▌        | 3/19 [00:11<01:04,  4.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.04s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:07<01:08,  4.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  11%|█         | 2/19 [00:08<01:08,  4.02s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:09<00:52,  3.28s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:05,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:05,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:05,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:05,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  16%|█▌        | 3/19 [00:12<01:05,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:12<00:49,  3.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:13<00:49,  3.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 4/19 [00:15<00:59,  3.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:16<00:46,  3.30s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.09s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.09s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.09s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.10s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.09s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  21%|██        | 4/19 [00:16<01:01,  4.09s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:16<00:46,  3.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:19<00:42,  3.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.02s/it]#015Loading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  26%|██▋       | 5/19 [00:19<00:56,  4.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:19<00:42,  3.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:56,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:57,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:57,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:56,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:57,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:56,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  26%|██▋       | 5/19 [00:20<00:57,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:22<00:38,  3.23s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:22<00:39,  3.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:23<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:23<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:23<00:52,  4.04s/it]#015Loading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  32%|███▏      | 6/19 [00:23<00:52,  4.04s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  32%|███▏      | 6/19 [00:24<00:52,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:26<00:35,  3.27s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:26<00:36,  3.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:27<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:27<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:27<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 7/19 [00:27<00:48,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:29<00:32,  3.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  37%|███▋      | 7/19 [00:28<00:48,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:29<00:33,  3.33s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:32<00:29,  3.30s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  42%|████▏     | 8/19 [00:32<00:44,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:32<00:29,  3.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:35<00:26,  3.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:36<00:26,  3.30s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  47%|████▋     | 9/19 [00:36<00:40,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:39<00:22,  3.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:39<00:22,  3.27s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  53%|█████▎    | 10/19 [00:40<00:36,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:42<00:19,  3.28s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:42<00:19,  3.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:45<00:16,  3.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:46<00:16,  3.31s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 12/19 [00:48<00:28,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [00:49<00:13,  3.31s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [00:49<00:13,  3.33s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [00:52<00:09,  3.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]#015Loading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  68%|██████▊   | 13/19 [00:52<00:24,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [00:52<00:10,  3.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [00:55<00:06,  3.23s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [00:55<00:06,  3.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.05s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  74%|███████▎  | 14/19 [00:56<00:20,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [00:58<00:03,  3.26s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [00:59<00:03,  3.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.12s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:01<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:01<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  79%|███████▉  | 15/19 [01:00<00:16,  4.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:01<00:00,  3.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:01<00:00,  3.25s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:02<00:00,  3.13s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:02<00:00,  3.26s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.14s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  3.95s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  3.95s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  4.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  4.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  4.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:11,  4.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  84%|████████▍ | 16/19 [01:04<00:12,  4.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:07,  3.91s/it]#015Loading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:07,  3.91s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:07,  3.87s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:07,  3.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:09<00:08,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:09<00:08,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:09<00:08,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:09<00:08,  4.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.13s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  89%|████████▉ | 17/19 [01:08<00:08,  4.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:11<00:03,  3.86s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:11<00:03,  3.86s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:12<00:03,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:12<00:03,  3.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.15s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:13<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  95%|█████████▍| 18/19 [01:12<00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.94s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.94s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.56s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:14<00:00,  3.95s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:15<00:00,  3.56s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:15<00:00,  3.95s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.01s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  3.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 19/19 [01:16<00:00,  4.02s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:27<00:27, 28.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:14<00:14, 14.86s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:14<00:14, 14.95s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.40s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:27<00:27, 27.73s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.50s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.15s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.60s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.55s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:13<00:13, 13.55s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:14<00:14, 14.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards:  50%|█████     | 1/2 [00:14<00:14, 14.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.36s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:22<00:00, 11.21s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:22<00:00, 11.50s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.37s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.70s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.40s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.73s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:24<00:00, 11.95s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:24<00:00, 12.40s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:37<00:00, 17.24s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.81s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:24<00:00, 11.96s/it]\u001b[0m\n",
      "\u001b[35mDownloading shards: 100%|██████████| 2/2 [00:24<00:00, 12.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:40<00:00, 18.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:40<00:00, 20.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.66s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:27<00:00, 13.34s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:27<00:00, 13.58s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.69s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.70s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:27<00:00, 13.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:27<00:00, 13.55s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.70s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:25<00:00, 12.80s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.23s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.91s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.06s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.03s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.10s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.33s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.45s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.63s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  8.00s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.35s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.26s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.69s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:53:688 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.43s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.82s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.41s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.80s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.48s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.87s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35malgo-2:59:692 [7] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:56:690 [4] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:52:688 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:58:691 [6] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:57:689 [5] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:53:693 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.58s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35malgo-2:55:694 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35malgo-2:54:695 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:56:689 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.80s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34malgo-1:57:690 [4] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.81s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.84s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.86s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34malgo-1:58:691 [5] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:54:692 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:55:693 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:59:694 [6] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34malgo-1:60:695 [7] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mmain()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "      File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mmain()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mFile \"/opt/ml/code/18.py\", line 195, in <module>\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "        training_function(args)teacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    main()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "        transformed_model = state.tp_registry.distribute(model, tp_config=config)\u001b[0m\n",
      "\u001b[35mmain()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    teacher_model = tsm.transform(teacher_model)  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    main()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[35mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)ValueError\u001b[0m\n",
      "\u001b[35m: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\u001b[0m\n",
      "\u001b[35mmain()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mValueError    training_function(args): \u001b[0m\n",
      "\u001b[35mTP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "    teacher_model = tsm.transform(teacher_model)  File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    training_function(args)\n",
      "      File \"/opt/ml/code/18.py\", line 154, in training_function\u001b[0m\n",
      "\u001b[35mteacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    teacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "      File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\u001b[0m\n",
      "\u001b[35mteacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mValueError:     TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mValueError    : main()TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.    \u001b[0m\n",
      "\u001b[35mtransformed_model = state.tp_registry.distribute(model, tp_config=config)  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    main()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[35mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\u001b[0m\n",
      "\u001b[35mtraining_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    teacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)    \u001b[0m\n",
      "\u001b[35mteacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      "    ValueErrortransformed_model = state.tp_registry.distribute(model, tp_config=config): \u001b[0m\n",
      "\u001b[35mTP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[35mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "    main()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "    main()  File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[34mtraining_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    teacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "    teacher_model = tsm.transform(teacher_model)    \u001b[0m\n",
      "\u001b[34mteacher_model = tsm.transform(teacher_model)  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "    training_function(args)  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")    \u001b[0m\n",
      "\u001b[34mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError\u001b[0m\n",
      "\u001b[34m: ValueErrorTP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[34mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      "    teacher_model = tsm.transform(teacher_model)\u001b[0m\n",
      "\u001b[34mValueError  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\u001b[0m\n",
      "\u001b[34m: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "    main()\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "    main()    \u001b[0m\n",
      "\u001b[34mtraining_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 191, in main\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      "    teacher_model = tsm.transform(teacher_model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    training_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\u001b[0m\n",
      "\u001b[34mtransformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    teacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      "    transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      "    raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[34mtraining_function(args)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/18.py\", line 154, in training_function\u001b[0m\n",
      "\u001b[34mteacher_model = tsm.transform(teacher_model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\u001b[0m\n",
      "\u001b[34mtransformed_model = state.tp_registry.distribute(model, tp_config=config)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\u001b[0m\n",
      "\u001b[34mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/18.py\", line 195, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/18.py\", line 191, in main\u001b[0m\n",
      "\u001b[34mtraining_function(args)\n",
      "  File \"/opt/ml/code/18.py\", line 154, in training_function\u001b[0m\n",
      "\u001b[34mteacher_model = tsm.transform(teacher_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\u001b[0m\n",
      "\u001b[34mtransformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\u001b[0m\n",
      "\u001b[34mraise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\u001b[0m\n",
      "\u001b[34mValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 53 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 54 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 55 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 56 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 58 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 59 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35m[2024-03-31 18:09:14,842] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 52) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[35msys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\u001b[0m\n",
      "\u001b[35mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\u001b[0m\n",
      "\u001b[35mrun(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\u001b[0m\n",
      "\u001b[35melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\u001b[0m\n",
      "\u001b[35mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\u001b[0m\n",
      "\u001b[35mraise ChildFailedError(\u001b[0m\n",
      "\u001b[35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35m18.py FAILED\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mFailures:\n",
      "  <NO_OTHER_FAILURES>\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[35m[0]:\n",
      "  time      : 2024-03-31_18:09:13\n",
      "  host      : algo-2\n",
      "  rank      : 8 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 52)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35m2024-03-31 18:09:15,068 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-03-31 18:09:15,068 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-03-31 18:09:15,069 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[35m2024-03-31 18:09:15,069 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mExitCode 1\u001b[0m\n",
      "\u001b[35mErrorMessage \"raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      " transformed_model = state.tp_registry.distribute(model, tp_config=config)ValueError\n",
      " TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      " main()\n",
      " File \"/opt/ml/code/18.py\", line 191, in main\n",
      " training_function(args)\n",
      " File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      " Traceback (most recent call last)\n",
      " ValueError    training_function(args)\n",
      " TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      " teacher_model = tsm.transform(teacher_model)  File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      " \n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      " teacher_model = tsm.transform(teacher_model)\n",
      " transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      " ValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " ValueError:     TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      " ValueError    : main()TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " transformed_model = state.tp_registry.distribute(model, tp_config=config)  File \"/opt/ml/code/18.py\", line 191, in main\n",
      " ValueErrortransformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      " [2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 53 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 54 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 55 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 56 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 58 closing signal SIGTERM\n",
      " [2024-03-31 18:09:13,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 59 closing signal SIGTERM\n",
      " [2024-03-31 18:09:14,842] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 52) of binary: /opt/conda/bin/python\n",
      " File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      " sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.ChildFailedError\n",
      " ============================================================\n",
      " 18.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " <NO_OTHER_FAILURES>\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " time      : 2024-03-31_18:09:13\n",
      " host      : algo-2\n",
      " rank      : 8 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 52)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\"\u001b[0m\n",
      "\u001b[35mCommand \"torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 18.py --alpha 0.5 --evaluation_strategy epoch --fp16 True --lr 6e-05 --mp_parameters hybrid_shard_degree=0,tensor_parallel_degree=8 --num_epochs 1 --save_strategy epoch --student_model_id mistralai/Mistral-7B-v0.1 --teacher_model_id mistralai/Mixtral-8x7B-v0.1 --temperature 4.0\"\u001b[0m\n",
      "\u001b[35m2024-03-31 18:09:15,069 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\u001b[34m[2024-03-31 18:09:18,766] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 53) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\u001b[0m\n",
      "\u001b[34mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\u001b[0m\n",
      "\u001b[34mrun(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors.\u001b[0m\n",
      "\u001b[34mChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m18.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 54)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 55)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 56)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[4]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 4 (local_rank: 4)\n",
      "  exitcode  : 1 (pid: 57)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[5]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 1 (pid: 58)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[6]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 59)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[7]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 1 (pid: 60)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2024-03-31_18:09:18\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 53)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m2024-03-31 18:09:19,012 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:09:19,012 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-31 18:09:19,012 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-03-31 18:09:19,012 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n",
      " ValueError\n",
      " ValueErrorTP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " teacher_model = tsm.transform(teacher_model)\n",
      " ValueError  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      " TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/ml/code/18.py\", line 195, in <module>\n",
      " transformed_model = state.tp_registry.distribute(model, tp_config=config)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n",
      " ValueError: TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n",
      " main()\n",
      " File \"/opt/ml/code/18.py\", line 191, in main\n",
      " training_function(args)\n",
      " File \"/opt/ml/code/18.py\", line 154, in training_function\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/parallelize.py\", line 45, in transform\n",
      " [2024-03-31 18:09:18,766] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 53) of binary: /opt/conda/bin/python\n",
      " File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      " sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.\n",
      " ChildFailedError\n",
      " ============================================================\n",
      " 18.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " [1]\n",
      " time      : 2024-03-31_18:09:18\n",
      " host      : algo-1\n",
      " rank      : 1 (local_rank: 1)\n",
      " exitcode  : 1 (pid: 54)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      " [2]\n",
      " rank      : 2 (local_rank: 2)\n",
      " exitcode  : 1 (pid: 55)\n",
      " [3]\n",
      " rank      : 3 (local_rank: 3)\n",
      " exitcode  : 1 (pid: 56)\n",
      " [4]\n",
      " rank      : 4 (local_rank: 4)\n",
      " exitcode  : 1 (pid: 57)\n",
      " [5]\n",
      " rank      : 5 (local_rank: 5)\n",
      " exitcode  : 1 (pid: 58)\n",
      " [6]\n",
      " rank      : 6 (local_rank: 6)\n",
      " exitcode  : 1 (pid: 59)\n",
      " [7]\n",
      " rank      : 7 (local_rank: 7)\n",
      " exitcode  : 1 (pid: 60)\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " rank      : 0 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 53)\"\u001b[0m\n",
      "\u001b[34mCommand \"torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 18.py --alpha 0.5 --evaluation_strategy epoch --fp16 True --lr 6e-05 --mp_parameters hybrid_shard_degree=0,tensor_parallel_degree=8 --num_epochs 1 --save_strategy epoch --student_model_id mistralai/Mistral-7B-v0.1 --teacher_model_id mistralai/Mixtral-8x7B-v0.1 --temperature 4.0\"\u001b[0m\n",
      "\u001b[34m2024-03-31 18:09:19,012 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-03-31 18:09:38 Uploading - Uploading generated training model\n",
      "2024-03-31 18:09:48 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job 18-2024-03-31-17-51-56-2024-03-31-17-51-56-423: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n transformed_model = state.tp_registry.distribute(model, tp_config=config)ValueError\n TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n main()\n File \"/opt/ml/code/18.py\", line 191, in main\n training_function(args)\n File \"/opt/ml/code/18.py\", line 154, in training_function\n Traceback (most recent call last)\n ValueError    training_function(args)\n TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n File \"/opt/ml/code/18.py\", line 195, in <module>\n teacher_model = tsm.transform(teacher_model)  File \"/opt/ml/code/18.py\", line 195, in <module>\n \n File \"/opt/conda/lib/python3.10/site-pa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpytorch_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFastFile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFastFile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1341\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2680\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5766\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5747\u001b[0m \n\u001b[1;32m   5748\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5764\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5766\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:7995\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7992\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   7994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 7995\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7996\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   7997\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:8048\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8044\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8045\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8046\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8047\u001b[0m     )\n\u001b[0;32m-> 8048\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8049\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8050\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8051\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8052\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job 18-2024-03-31-17-51-56-2024-03-31-17-51-56-423: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(f\"TP policy does not exist for {type(module)}, can't transform.\")\n transformed_model = state.tp_registry.distribute(model, tp_config=config)ValueError\n TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.  File \"/opt/conda/lib/python3.10/site-packages/torch/sagemaker/tensor_parallel/tp_registry.py\", line 48, in distribute\n main()\n File \"/opt/ml/code/18.py\", line 191, in main\n training_function(args)\n File \"/opt/ml/code/18.py\", line 154, in training_function\n Traceback (most recent call last)\n ValueError    training_function(args)\n TP policy does not exist for <class 'transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification'>, can't transform.\n File \"/opt/ml/code/18.py\", line 195, in <module>\n teacher_model = tsm.transform(teacher_model)  File \"/opt/ml/code/18.py\", line 195, in <module>\n \n File \"/opt/conda/lib/python3.10/site-pa"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit(inputs={\n",
    "    'train': TrainingInput(\n",
    "        s3_data=train_dataset_path,\n",
    "        input_mode='FastFile'),\n",
    "    'test': TrainingInput(\n",
    "        s3_data=test_dataset_path,\n",
    "        input_mode='FastFile'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c3ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
